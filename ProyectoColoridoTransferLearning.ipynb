{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProyectoColoridoTransferLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValBrain/models/blob/master/ProyectoColoridoTransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTAyyV28gcUu",
        "colab_type": "text"
      },
      "source": [
        "# **Librerías**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN37_8rqgeft",
        "colab_type": "code",
        "outputId": "2b24c6b8-c74d-408e-f964-d4b594137bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from skimage import io, color\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "import glob\n",
        "import cv2\n",
        "import h5py\n",
        "import json\n",
        "import datetime\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXxXwIdavS-H",
        "colab_type": "text"
      },
      "source": [
        "### Se crea la sesión de tensor flow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa6U-myYvSQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.set_random_seed(1)\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs-ndFqbgfuP",
        "colab_type": "text"
      },
      "source": [
        "# **Obtención dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZhRigGOwnMS",
        "colab_type": "code",
        "outputId": "b480db62-ea6f-44a3-f423-dca311384961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Se clona repositorio con las fotos\n",
        "!git clone https://ValBrain:valentinagianluca@github.com/ValBrain/Proyecto-EL4106.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Proyecto-EL4106'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 551 (delta 11), reused 4 (delta 0), pack-reused 522\u001b[K\n",
            "Receiving objects: 100% (551/551), 905.31 KiB | 835.00 KiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU5qnePtgkkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se obtienen las imagenes en formato LAB (RGB to LAB) y se guardan en una lista\n",
        "dirname = 'Proyecto-EL4106/tiny-imagenet/n09246464/n09246464_'\n",
        "dataSet = []\n",
        "for i in range(500):\n",
        "  rgb = io.imread(dirname+str(i)+'.JPEG')\n",
        "  if rgb.shape == (64, 64, 3):\n",
        "    lab = color.rgb2lab(rgb)\n",
        "    dataSet.append(lab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Srq_AvdxOGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se separa L de AB\n",
        "dataL = []\n",
        "dataAB = []\n",
        "for i in range(len(dataSet)):\n",
        "  dataL.append(dataSet[i][:,:,0])\n",
        "  dataAB.append(dataSet[i][:,:,1:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUUnF5yywsP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se convierte en tensores\n",
        "arrayDataSet = np.asarray(dataSet)\n",
        "arrayDataL = np.asarray(dataL)\n",
        "arrayDataAB = np.asarray(dataAB)\n",
        "tensorDataSet = tf.convert_to_tensor(arrayDataSet, tf.float32) # Set original en LAB\n",
        "tensorDataL = tf.convert_to_tensor(arrayDataL, tf.float32) # L del set original\n",
        "tensorDataAB = tf.convert_to_tensor(arrayDataAB, tf.float32) # AB del set original"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1pKS1x11oES",
        "colab_type": "text"
      },
      "source": [
        "# Construcción del Modelo "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x8IjkNy1x3r",
        "colab_type": "text"
      },
      "source": [
        "## Funciones utiles \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjSg6-tA1wg7",
        "colab_type": "code",
        "outputId": "983facd1-b0d9-4467-f8c4-1457a4ad3c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "#FUNCIONES\n",
        "#def colorizacion():\n",
        "\n",
        "def conv_layer_1(input_tensor, kernel_shape, layer_name):\n",
        "  \n",
        "  weights = tf.get_variable('weights', kernel_shape)\n",
        "  biases = tf.get_variable('biases', [kernel_shape[3]], initializer=tf.constant_initializer(0.05))\n",
        "  \n",
        "  tf.summary.histogram(layer_name + \"/weights\", weights)\n",
        "  tf.summary.histogram(layer_name + \"/biases\", biases)\n",
        "  \n",
        "  conv = tf.nn.conv2d(input_tensor, weights, stride = [1,1,1,1], padding = 'SAME')\n",
        "  \n",
        "  return tf.nnrelu(conv + biases)\n",
        "  \n",
        "def conv_layer_2(input_tensor, kernel_shape, layer_name):\n",
        "  \n",
        "  weights = tf.get_variable('weights', kernel_shape)\n",
        "  biases = tf.get_variable('biases', [kernel_shape[3]], initializer=tf.constant_initializer(0.05))\n",
        "  \n",
        "  tf.summary.histogram(layer_name + \"/weights\", weights)\n",
        "  tf.summary.histogram(layer_name + \"/biases\", biases)\n",
        "  \n",
        "  conv = tf.nn.conv2d(input_tensor, weights, stride = [1,2,2,1], padding = 'SAME')\n",
        "  \n",
        "  return tf.nn.relu(conv + biases)\n",
        "\n",
        "def conv_layer_final(input_tensor, kernel_shape, layer_name):\n",
        "  weights = tf.get_variable('weights', kernel_shape)\n",
        "  biases = tf.get_variable('biases', [kernel_shape[3]], initializer=tf.constant_initializer(0.05))\n",
        "  \n",
        "  tf.summary.histogram(layer_name + \"/weights\", weights)\n",
        "  tf.summary.histogram(layer_name + \"/biases\", biases)\n",
        "  \n",
        "  conv = tf.nn.conv2d(input_tensor, weights, stride = [1,1,1,1], padding = 'SAME')\n",
        "  \n",
        "  return tf.nn.tanh(conv + biases)\n",
        "\n",
        "def feature_extractor(input_tensor):\n",
        "  model_input_resnet = tf.placeholder(tf.int8, name='model_input_resnet', \n",
        "                             shape=(299, 299, 3))\n",
        "  base_model = InceptionResNetV2(include_top=True, weights='imagenet', input_tensor='model_input_resnet', input_shape=None, pooling=None, classes=1000)\n",
        "\n",
        "  \n",
        "  model = Model(input=base_model.input, output=base_model.get_layer('predictions').output)\n",
        "  \n",
        "  #return base_model\n",
        "  print(tf.shape(base_model))\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-dd5d45406153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: feature_extractor() missing 2 required positional arguments: 'input_tensor' and 'layer_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuiXdFfZmy8X",
        "colab_type": "code",
        "outputId": "b92460de-b39a-4259-d27e-9827ec965701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "#### Probando feacture extractor\n",
        "\n",
        "input_tensor = tensorDataL[3].set_shape([299,299])\n",
        "\n",
        "\n",
        "tf.image.resize_with_pad(\n",
        "    image,\n",
        "    target_height,\n",
        "    target_width,\n",
        "    method=ResizeMethod.BILINEAR,\n",
        "    antialias=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(input_tensor)\n",
        "\n",
        "#base_model = feature_extractor(input_tensor, 'feactures_extractor')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    571\u001b[0m           \u001b[0mdim_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m           unknown_shape)\n\u001b[0m\u001b[1;32m    573\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 64 and 299. Shapes are [64,64] and [299,299].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-efc8572b85fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorDataL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m       \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 64 and 299. Shapes are [64,64] and [299,299]."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYgAcmKq2JaF",
        "colab_type": "text"
      },
      "source": [
        "## Arquitectura del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtJtlT1M2QU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#def encoder_layers(input_tensor, kernel_shape, layer_name):\n",
        "n_conv_layers = 8\n",
        "conv_layers = [64, 128, 128, 256, 256, 512, 512, 256]\n",
        "model_input = tf.placeholder(tf.float32, name='model_input', \n",
        "                           shape=(batch_size, 224, 224, 1))\n",
        "\n",
        "layer_input = model_input\n",
        "previous_n_feature_maps = 3\n",
        "for layer_index in range(n_conv_layers):\n",
        "    layer_name = 'conv%d' % layer_index\n",
        "    if (layer_index == 0) or (layer_index == 2) or (layer_index==4):\n",
        "      with tf.variable_scope(layer_name):\n",
        "        conv_out = conv_layer_2(\n",
        "            layer_input, \n",
        "            [3, 3, previous_n_feature_maps, n_filters_convs[layer_index]], \n",
        "            layer_name)\n",
        "    if True and layer_index == 0:\n",
        "      with tf.variable_scope(layer_name, reuse=True):\n",
        "          conv_filters_1 = tf.get_variable(\"weights\")\n",
        "          tf.summary.image(\n",
        "              'conv_filters_1',\n",
        "              tf.transpose(conv_filters_1, perm=[3, 0, 1, 2]),\n",
        "              max_outputs=n_filters_convs[layer_index])\n",
        "\n",
        "    else:\n",
        "      with tf.variable_scope(layer_name):\n",
        "        conv_out = conv_layer_2(\n",
        "            layer_input, \n",
        "            [3, 3, previous_n_feature_maps, n_filters_convs[layer_index]], \n",
        "            layer_name)\n",
        "            previous_n_feature_maps = n_filters_convs[layer_index]\n",
        "\n",
        "    print('Encoder conv outs:',conv_out)\n",
        "    layer_input = conv_out#[layer_index]\n",
        "    previous_n_feature_maps = n_filters_convs[layer_index]\n",
        "\n",
        "    print('Encoder last layer:',layer_input)\n",
        "\n",
        "\n",
        "#def fusion_layers(input_tensor_1 , input_tensor_2 , kernel_shape_1, kernel_shape_2, layer_name):\n",
        "\n",
        "layer_name = 'conv_1_1'\n",
        "with tf.variable_scope(layer_name):\n",
        "  filter = tf.get_variable('weights', [1, 1, previous_n_feature_maps, 256], initializer=tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32), dtype=tf.float32)\n",
        "  conv_out = conv_layer_1(layer_input, filter,[1, 1, 1, 1], padding='SAME', name= layer_name)\n",
        "\n",
        "#tomar ultimo featuremap y concatenar el vector nuevo \n",
        "  #concatenar los vectores de resnet\n",
        "  \n",
        "#def decoder_layers():\n",
        "previous_n_feature_maps = ##############################################\n",
        "layer_input = \n",
        "n_conv_layers_2 = 5\n",
        "conv_layers_2 = [32, 64, 64, 128, 256]\n",
        "for layer_index in range(n_conv_layers_2):\n",
        "    layer_name = 'conv%d' % layer_index\n",
        "    if (layer_index == n_conv_layers_2):\n",
        "      with tf.variable_scope(layer_name):\n",
        "        conv_out = conv_layer_final(\n",
        "            layer_input, \n",
        "            [3, 3, previous_n_feature_maps, n_filters_convs[layer_index]], \n",
        "            layer_name)\n",
        "    if True and layer_index == 0:\n",
        "      with tf.variable_scope(layer_name, reuse=True):\n",
        "          conv_filters_2 = tf.get_variable(\"weights\")\n",
        "          tf.summary.image(\n",
        "              'conv_filters_2',\n",
        "              tf.transpose(conv_filters_2, perm=[3, 0, 1, 2]),\n",
        "              max_outputs=n_filters_convs[layer_index])\n",
        "\n",
        "    else:\n",
        "      with tf.variable_scope(layer_name):\n",
        "        conv_out = conv_layer_1(\n",
        "            layer_input, \n",
        "            [3, 3, previous_n_feature_maps, n_filters_convs[layer_index]], \n",
        "            layer_name)\n",
        "            previous_n_feature_maps = n_filters_convs[layer_index]\n",
        "    \n",
        "    model_output = conv_out #[layer_index]\n",
        "    \n",
        "with tf.name_scope('loss_function'):\n",
        "            probabilities = tf.nn.softmax(logits)\n",
        "            loss = tf.reduce_mean(\n",
        "                tf.square(model_output - target),\n",
        "                name='mse'\n",
        "            )\n",
        "            # Summary de loss\n",
        "            loss_sum = tf.summary.scalar('mse_loss', loss)\n",
        "\n",
        "  #función que entrega la imagen con la función de costos aplicada\n",
        "    \n",
        "#model = InceptionResNetV2()\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEPV5xYP2jgj",
        "colab_type": "code",
        "outputId": "27933c9d-d494-4038-8df4-41cd8ed8479d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"concat_7:0\", shape=(4, 3), dtype=int32)\n",
            "Tensor(\"concat_8:0\", shape=(2, 6), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}